# -*- coding: utf-8 -*-
"""modelo_asistente_virtual_para_ciencia_abierta_version_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zyhLUx-t0IUaXEi2FqdBySuZHMy3yP1j

# 1. Librerías utilizadas.
"""

# # Lectura e ingesta de PDFs
# os
# pdfplumber==0.11.7

# # #Limpieza y Procesamiento de documentos
# # spacy==3.8.7
# nltk==3.9.1
# pandas==2.2.2
# regex==2024.11.6


# # Embeddings y vectorización
# sentence-transformers==4.1.0
# chromadb==1.0.15    (base de datos vectorial)
# faiss-cpu ==1.7.4

# # Framework RAG (Seleccionar langchain o llama-index no es necesario utilizar las dos)
# langchain==0.3.27
# langchain-community==0.3.27
# langchain-core==0.3.72
# langchain-text-splitters==0.3.9
# rank-bm25==0.2.2
# llama-index ==0.9.0.

# # APIs LLM (Seleccionar openai o anthropic de claude no es necesario utilizar las dos)
# huggingface-hub==0.34.3
# openai==1.98.0
# transformers==4.54.1
# torch @ https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl

# # Interfaz y API (Para más adelante, elegir gradio o streamlit, no es necesario utilizar las dos)
# streamlit==1.28.0
# gradio==4.8.0  (utilizar esta por su sencillez, luego en la iteración agregar streamlit)

"""## 1.1 Instalación de librerías"""

!pip install pdfplumber spacy sentence_transformers -U langchain-community chromadb rank_bm25
!pip install -U :class:`~langchain-huggingface.HuggingFaceEmbeddings

"""## 1.2 Importación de librerías"""

# Librerías principales de python
import pdfplumber # Extracción de texto PDF
import spacy
import sentence_transformers  # Embeddings semánticos
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path
# import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2") # Use model_name directly

"""# 2. Pipeline de Procesamiento de Documentos

## 2.1 Ingesta y Preprocesamiento de los archivos PDFs
"""

import os
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() + "\n"
    except Exception as e:
        print(f"Error extracting text from {pdf_path}: {e}")
    return text

def split_text_into_chunks(text, chunk_size=500, chunk_overlap=100):
    """Splits text into smaller chunks."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    chunks = text_splitter.split_text(text)
    return chunks

# Define the path to your PDF folder
input_folder = "pdfs"
folder_path = Path(input_folder)

# Check if the folder exists
if not folder_path.exists():
    print(f"Error: Folder '{input_folder}' not found.")
    # Create the folder if it doesn't exist and add a placeholder file
    try:
        folder_path.mkdir(parents=True, exist_ok=True)
        print(f"Carpeta creada: '{input_folder}'. Por favor subí tus archivos pdfs aquí.")
        # # Create a placeholder file to indicate where to put PDFs
        # placeholder_file = folder_path / "upload_your_pdfs_here"
        # with open(placeholder_file, "w") as f:
        #     f.write("Por favor subí tus archivos pdfs a esta carpeta.")
    except Exception as e:
        print(f"Error creating folder '{input_folder}': {e}")
    pdf_files = [] # Set pdf_files to empty list if folder doesn't exist
else:
    # Get a list of all PDF files in the folder
    pdf_files = list(folder_path.glob("*.pdf"))

all_document_text = ""
if pdf_files:
    print(f" {len(pdf_files)} PDF files in '{input_folder}'. Processing...")
    for pdf_file in pdf_files:
        print(f"Processing: {pdf_file}")
        document_text = extract_text_from_pdf(pdf_file)
        if document_text:
            all_document_text += document_text + "\n"
    print("Finished processing PDF files.")

    # Split the combined text into chunks
    if all_document_text:
        text_chunks = split_text_into_chunks(all_document_text)
        print(f"Successfully extracted and split {len(text_chunks)} chunks from all documents.")
    else:
        print("No text extracted from any PDF files.")
        text_chunks = []

else:
    print(f"No PDF files found in '{input_folder}'. Please upload your PDF files to this folder.")
    text_chunks = [] # Initialize as empty list if no files are found

# Now text_chunks contains all chunks from all processed PDFs

"""## 2.2 Procesamiento de pdfs

### Consta de las siguientes etapas:

* El documento ya se segmento en chunks de 500-1000 tokens, a continuación hay que tokenizar y luegar aplicar embeddings.
* Limpieza y normalización de texto en español

"""

import re
import spacy
from typing import List, Dict, Tuple
import pandas as pd
from collections import defaultdict

"""## 2.2.1 Descarga de recursos necesarios de Spacy"""

# Cargar modelo de spaCy para español
try:
   nlp = spacy.load("es_core_news_sm")
   print("Modelo de spaCy cargado.")
except OSError:
  print("Modelo de spaCy no encontrado. Se instalará a continuación.")
  !python -m spacy download es_core_news_sm
  nlp = spacy.load("es_core_news_sm")
  #  print("Modelo de spaCy no encontrado. Instalar con: python -m spacy download es_core_news_sm")
  #  nlp = None

"""### 2.2.2 Funciones para el procesamiento de los PDFs

### Limpieza del texto
<!-- Verificar si aplica stopwords -->
"""

import re
import spacy
from typing import List, Dict, Tuple
import pandas as pd
from collections import defaultdict
# import nltk   si utilizo spacy no es necesario utilizar NTLK
# from nltk.tokenize import sent_tokenize, word_tokenize

class DocumentProcessor:
   def __init__(self, chunk_size_tokens=500, chunk_overlap_tokens=100):
       self.chunk_size_tokens = chunk_size_tokens
       self.chunk_overlap_tokens = chunk_overlap_tokens
       self.nlp = nlp


   def clean_and_normalize_spanish(self, text: str) -> str:  # podría agregar algunas reglas de limpieza del  Código del CORPUS
       """
       Limpieza y normalización de texto en español
       """
       # Normalizar caracteres especiales del español
       replacements = {
           'á': 'á', 'é': 'é', 'í': 'í', 'ó': 'ó', 'ú': 'ú',
           'ñ': 'ñ', 'ü': 'ü', 'Á': 'Á', 'É': 'É', 'Í': 'Í',
           'Ó': 'Ó', 'Ú': 'Ú', 'Ñ': 'Ñ', 'Ü': 'Ü'
       }

       # Normalizar espacios y saltos de línea
       text = re.sub(r'\s+', ' ', text)
       text = re.sub(r'\n+', '\n', text)

       # Eliminar caracteres de control
       text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)

       # Normalizar puntuación
       text = re.sub(r'["""]', '"', text)
       text = re.sub(r"['']", "'", text) # Fixed the regex pattern for single quotes
       text = re.sub(r'[–—]', '-', text)

       # Eliminar espacios antes de puntuación
       text = re.sub(r'\s+([,.;:!?])', r'\1', text)

       # Normalizar números y fechas
       text = re.sub(r'(\d+)\s*,\s*(\d+)', r'\1,\2', text)
       text = re.sub(r'(\d+)\s*\.\s*(\d+)', r'\1.\2', text)

       # Eliminar URLs y emails (preservar contexto)
       text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-Z]))+', '[URL]', text)
       text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
       text = re.sub(r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-Z]))+', '', text)

       # Limpiar espacios múltiples
       text = re.sub(r'\s+', ' ', text)

      #---------------------------------------------
      # LIMPIEZA AGREGADA DESDE EL CODIGO DE CORPUS
      #---------------------------------------------

       # Eliminar números de página (patrones comunes)
      # Elimina números solos en una línea o números con formato "Página X"
       text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)
       text = re.sub(r'página\s*\d+', '', text, flags=re.IGNORECASE)
       text = re.sub(r'pp' '\s*\d+', '', text, flags=re.IGNORECASE)
       text = re.sub(r'p\.\s*\d+', '', text, flags=re.IGNORECASE)
       text = re.sub(r'pág\.\s*\d+', '', text, flags=re.IGNORECASE)
       text = re.sub(r'^-\s*\d+\s*-\s*$', '', text, flags=re.MULTILINE)

        # También manejar casos donde hay espacios antes del salto de línea
        # Como "nac- ieron" en la misma línea
       text = re.sub(r'(\w+)-\s+(\w+)', r'\1\2', text)

        # Eliminar patrones tipo peerj.preprints.27580v1 o similares
        # Este patrón captura: palabra.palabra.número+letra+número
       text = re.sub(r'\b\w+\.\w+\.\d+[a-zA-Z]+\d*\b', ' ', text)

        # Eliminar patrones de identificadores de documentos científicos
        # Como: arxiv.1234.5678, doi.10.1234/5678, pmid.12345678
       text = re.sub(r'\b\w+\.\d+\.\d+\b', ' ', text)
       text = re.sub(r'\bdoi\.\borg\.\S+', ' ', text)
       text = re.sub(r'\bpmid\.\d+', ' ', text)
       text = re.sub(r'\barxiv\.\d+\.\d+', ' ', text)


        # Eliminar números con comas como 45, 56, 78
        # Este patrón elimina secuencias de números separados por comas
       text = re.sub(r'\b\d+(?:\s*,\s*\d+)+\b', '', text)

        # Eliminar números solos (incluyendo decimales)
        # Esto elimina números aislados como 123, 45.67, etc.
       text = re.sub(r'\b\d+\.?\d*\b', '', text)

        # Eliminar referencias tipo [1], [2,3], [45-47], etc.
       text = re.sub(r'\[\d+(?:[-,]\d+)*\]', '', text)

        # Eliminar años solos (4 dígitos)
       text = re.sub(r'\b\d{4}\b', '', text)

        # casos especiales de limpieza del texto de UNESCO
        #Eliminar números romanos en minúscula
        #roman_numerals_pattern = r'\b(?:i[vx]|v?i{0,3}|x)\b'
        # elimina numeracion del tipo 45ª
       text = re.sub(r'\b\d+ª\b', ' ', text)
        #elimina url que solo terminan con '.com'
       text = re.sub(r'\b(?!https:\/\/|www\.)[a-zA-Z0-9.-]+\.com\b', ' ', text)


        # casos especiales de limpieza del texto de Conocimiento abierto
        # elimina letras solas
       text = re.sub(r'\b[a-z]\b', ' ', text)
        # elimina el siguiente patrón: ccoolleecccciióónn ggrruuppooss ddee ttrraabbaajjoo ccoonnoocciimmiieennttoo aabbiieerrttoo eenn aamméérriiccaa llaattiinnaa
       text = re.sub(r'(?<!acc)(?<!desarroll)([a-záéíóúñ])\1+(?!eso)(?!lo)',r'\1', text) # ver si conviene o no, tiene palabras en ingles como access(queda aces) o common(queda comon)
        # filtra las palabras de 2 digitos: Ej:fs, ar, eb
       text = re.sub(r'\b[a-z]{2}\b', ' ', text)
        # filtra las palabras de 3 digitos sin vocales: Ej:pcb
       text = re.sub(r'\b[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]{3}\b', ' ', text)
        # filtra las asociaciones de este tipo: n2o0s2, a0i
       text = re.sub(r'\b(?=\w*\d)(?=\d*\w)\w+\b', ' ', text)

        # Remove HTML tags
       text = re.sub ( r'<.*?>' ,'', text)

        # Eliminar correos electrónicos
       text = re.sub(r'\S+@\S+', '', text)

        # Eliminar caracteres especiales excesivos pero mantener puntuación básica
        # Mantener: . , ; : ! ? ¿ ¡ ( ) - " '
        # text = re.sub(r'[^\w\s\.\,\;\:\!\?\¿\¡\(\)\-\"\ª\'áéíóúñÁÉÍÓÚÑ]', '', text)
        # decido eliminar todos los caracteres especiales por eso comento lo anterior
       text = re.sub(r'[^\w\s]', '', text)

        # Eliminar múltiples espacios
       text = re.sub(r'\s+', ' ', text)

        # Eliminar múltiples saltos de línea
       text = re.sub(r'\n+', '\n', text)

        # Eliminar líneas que solo contienen espacios
       lines = text.split('\n')
       lines = [line.strip() for line in lines if line.strip()]
       text = '\n'.join(lines)

       # Eliminar stopwords de spaCy
       if self.nlp:
            doc = self.nlp(text)
            text = " ".join([token.text for token in doc if not token.is_stop])

       return text.strip()

"""## 2.2.3 Tokenización de los Chunks con spaCY

Forma sencilla de generar los tokens de los chunks de texto para luego proceder a convertirlos a embeddings. En la versión original los embeddings se generan directamente desde los trozos de chunks.
"""

# Tokenizar los chunks usando spaCy
if text_chunks and nlp:
    tokenized_chunks = []
    for chunk in text_chunks:
        doc = nlp(chunk)
        tokens = [token.text for token in doc]
        tokenized_chunks.append(tokens)
    print(f"Successfully tokenized {len(tokenized_chunks)} chunks.")
elif not nlp:
    print("SpaCy model not loaded. Skipping tokenization.")
else:
    print("No text chunks available for tokenization.")

# Now you can proceed to generate embeddings from the tokenized_chunks or directly from text_chunks as before.
# The original embedding generation code uses the raw text chunks.

"""## 2.3 Generación de Embeddings: esto mismo se hace en el punto 4, entonces no es necesario hacerlo aquí"""

# # Modelo multilingüe especializado de SentenceTransformer
# from sentence_transformers import SentenceTransformer
# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

# # Generación de representaciones vectoriales
# if tokenized_chunks: # Only generate embeddings if text_chunks is not empty  # En esta linea cambié tokenized_chunks(version anterior) POR text_chunks
#     embeddings = model.encode(tokenized_chunks,
#                              batch_size=32,
#                              show_progress_bar=True)
#     print("Embeddings generated successfully.")
# else:
#     print("No text chunks to generate embeddings from.")
#     embeddings = None # Initialize embeddings as None if no chunks were created

"""## Limpieza con stopwords de Spacy"""

# Create an instance of the DocumentProcessor
# Assuming nlp (spaCy model) is already loaded
processor = DocumentProcessor()

# Apply the cleaning and normalization function to each text chunk
if text_chunks:
    cleaned_text_chunks = [processor.clean_and_normalize_spanish(chunk) for chunk in text_chunks]
    print(f"Successfully cleaned and normalized {len(cleaned_text_chunks)} text chunks.")
else:
    cleaned_text_chunks = []
    print("No text chunks available to clean.")

# Now cleaned_text_chunks contains the processed text
# You can inspect the first few cleaned chunks
# for i, cleaned_chunk in enumerate(cleaned_text_chunks[:5]):
#     print(f"\n--- Cleaned Chunk {i+1} ---")
#     print(cleaned_chunk)

"""# 3. Base de Conocimiento Estructurada (en desarrollo)

## 3.1 Taxonomía de Pilares de Ciencia Abierta

Para la definición de la taxonomÍa utilizada en el entrenamiento se revisaron los siguientes documentos, recomendaciones de CA de la UNESCO (2021) y  Taxonomia da Ciência Aberta: revisada e ampliada, además del material del su curso de OpenScience 101 desarrollado por por Nasa. En estos se identificaron distintas taxonomías como Pontika et al. (2015), Baumgartner (2019), Silveira et al. (2021), y una última ampliada. Finalmente, se decidió establecer una clasificación basada en las recomendaciones de la Unesco para la ciencia abierta y en los módulos definidos en el curso de OpenScience 101. La misma consiste en: acceso_abierto.datos abiertos, codigo_abierto, evaluacion_abierta y ciencia_ciudadana.

Fuente: Artículo: SciELO Brasil - Taxonomia da Ciência Aberta: revisada e ampliada Taxonomia da Ciência Aberta: revisada e ampliada https://share.google/C0r8ikdeLFwAGjR2u.
"""

PILARES_CA = {
    "acceso_abierto": {
        "subcategorias": ["publicaciones", "repositorios", "preprints"],
        "entidades": ["revistas", "editoriales", "licencias"]
    },
    "datos_abiertos": {
        "subcategorias": ["FAIR", "repositorios", "metadatos"],
        "entidades": ["formatos", "identificadores", "planes_gestion"]
    },
    "codigo_abierto": {
        "subcategorias": ["software", "notebooks", "reproducibilidad"],
        "entidades": ["licencias", "plataformas", "versionado", "lenguaje"]
    },
    "evaluacion_abierta": {
        "subcategorias": ["peer_review", "metricas", "transparencia"],
        "entidades": ["altmetrics", "sistemas_evaluacion"]
    },
    "ciencia_ciudadana": {
        "subcategorias": ["participacion", "metodologias", "plataformas"],
        "entidades": ["proyectos", "herramientas", "comunidades"]
    }
}

"""## 3.2 Estructura de Pares Pregunta-Respuesta(en desarrollo)"""

class QAPair:
    def __init__(self):
        self.pregunta = ""
        self.respuesta = ""
        self.pilar = ""
        self.nivel_complejidad = ""  # básico, intermedio, avanzado
        self.audiencia = ""  # maestría, doctorado, investigador
        self.contexto_regional = ""  # América Latina específico
        self.fuentes = []
        self.entidades_relacionadas = []

"""# 4. Arquitectura de Retrieval

## 4.1 Sistema de Búsqueda Multimodal

Hasta acá se han extraído fragmentos de texto desde los pdfs y luego se han generado los correspondientes embeddings. Los pasos a continuación son:

* creación de objetos del tipo Document a partir de los fragmentos de texto.
* creación de un vector_store (utilizando Chroma) a partir de los objetos Document y sus correspondientes embeddings. Por último, se utilizarán los documentos y el vector_store para configurar su BM25Retriever, VectorStoreRetriever y EnsembleRetriever.

En sintesis, este código prepara los datos de texto para su recuperación convirtiéndolos a un formato (documentos con incrustaciones almacenadas en una base de datos vectorial) que permite buscar de manera eficiente información relevante basada en la similitud semántica. Este es un paso fundamental en un proceso RAG, ya que permite encontrar las partes más relevantes de los documentos para responder a las consultas de los usuarios.
"""

# IMPORTANTE: Revisar quizás no es necesario realizar la conversión a embeddings de los pasos previos porque en este paso toma los trozos de texto generados anteriormente, arma objetos de tipo documentos
# y le aplica una función de embeddings para guardar en la base de chroma el texto vectorizado.

from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings # Import HuggingFaceEmbeddings

# Create Document objects from cleaned_text_chunks
if text_chunks is not None:   # ver si conviene utilizar limpio o no cleaned_text_chunks
    documents = [Document(page_content=chunk) for chunk in text_chunks]  # verificar si utilizo el texto limpio o directamente text_chunks porque quizás necesito el texto completo para el objetivo de este documento
    print(f"Created {len(documents)} Document objects from cleaned chunks.")
else:
    documents = []
    print("No cleaned text chunks available to create documents.")

# Assuming you have already loaded the model
# If not, you need to run the cell that loads the model (cell id: 4417745d)
if documents and model is not None: # Check if documents and the model are available
    # Create a HuggingFaceEmbeddings object using your SentenceTransformer model
    embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2") # Use model_name directly

    # Create a Chroma vector store, passing the embedding function
    vector_store = Chroma.from_documents(documents, embedding_function)
    print("Chroma vector store created from documents.")
else:
    vector_store = None
    print("Could not create Chroma vector store: no documents or embedding model available.")

#para aplicar lo siguiente antes debo cargar los objetos de tipo Documents
from langchain.vectorstores import Chroma
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# Retriever híbrido
bm25_retriever = BM25Retriever.from_documents(documents)
vector_retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.7, "k": 5}
)

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.3, 0.7]
)

"""# 5. Integración con APIs de LLM

La clase LLMProvider permite cambiar fácilmente entre diferentes proveedores de LLM especificando el nombre del proveedor al crear una instancia de la clase, sin tener que modificar el código que utiliza el cliente. Esto es útil para mantener el código flexible y experimentar con diferentes modelos.
"""

class LLMProvider:
    def __init__(self, provider="openai"):
        self.provider = provider
        self.client = self._initialize_client()

    def _initialize_client(self):
        if self.provider == "openai":
            return OpenAI(api_key=os.getenv("KEY_OPENAI_API"))
        elif self.provider == "anthropic":
            return Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        elif self.provider == "huggingface":
            return HuggingFaceHub(repo_id="microsoft/DialoGPT-large")

"""## 5.1 Prompt Engineering Especializado"""

SYSTEM_PROMPT = """
Eres un asistente especializado en Ciencia Abierta para investigadores y estudiantes de posgrado de América Latina.

Tu conocimiento se basa en:
- Recomendación UNESCO sobre Ciencia Abierta (2021)
- Recursos de Redalyc
- Documentos especializados en español
- Contexto regional latinoamericano
- Mejores prácticas académicas utilizadas en América Latina

Responde de manera:
- Precisa y basada en evidencia
- Adaptada al nivel del usuario: básico, intermedio, avanzado
- Con ejemplos del contexto latinoamericano
- Incluyendo fuentes y recursos adicionales

Contexto recuperado: {context}
Perfil del usuario: {user_profile}
"""

"""# 6. Pipeline de Entrenamiento y Optimización
## 6.1 Fine-tuning del Modelo de Embeddings: para la evaluación del modelo.

Este código está diseñado para fine-tune (ajustar) un modelo de embeddings pre-entrenado (sentence-transformers/paraphrase-multilingual-mpnet-base-v2) utilizando ejemplos específicos de tu dominio (Ciencia Abierta en español). El objetivo es mejorar la capacidad del modelo para generar embeddings que capturen mejor la similitud semántica entre textos relacionados con este tema.
En resumen, este código toma un modelo de embeddings pre-entrenado y lo adapta a tu dominio específico (Ciencia Abierta) usando un pequeño conjunto de ejemplos de entrenamiento, con el objetivo de mejorar la calidad de los embeddings para las tareas relacionadas con tus documentos.
"""

from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation
from torch.utils.data import DataLoader
import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Ejemplos de dominio-específico
examples = [
    InputExample(texts=['¿Qué es el acceso abierto?', 'El acceso abierto es una práctica que permite a los usuarios leer, descargar, copiar, distribuir, imprimir, buscar o enlazar los textos completos de artículos científicos y de investigación y utilizarlos para cualquier otro fin legítimo, sin barreras financieras, legales o técnicas, con la única salvedad de las que supone Internet misma.'], label=1.0),
    InputExample(texts=['Datos FAIR', 'Los principios FAIR (Findable, Accessible, Interoperable, Reusable) son un conjunto de directrices para la gestión y administración de datos de investigación, asegurando que los datos sean localizables, accesibles, interoperables y reutilizables tanto por humanos como por máquinas.'], label=1.0),
    InputExample(texts=['¿Cómo se relaciona la ciencia ciudadana con la ciencia abierta?', 'La ciencia ciudadana es una faceta de la ciencia abierta que permite la participación del público en general en proyectos de investigación científica.'], label=1.0),
    InputExample(texts=['¿Qué es la evaluación abierta?', 'La evaluación abierta se refiere a la revisión por pares y la evaluación de la investigación que es transparente y participativa.'], label=1.0),
    InputExample(texts=['Repositorios de acceso abierto', 'Los repositorios son plataformas donde los investigadores pueden depositar sus publicaciones y datos para que estén disponibles en acceso abierto.'], label=1.0),
    InputExample(texts=['Licencias Creative Commons', 'Las licencias Creative Commons son un conjunto de herramientas legales que permiten a los creadores compartir su trabajo de forma abierta, definiendo cómo otros pueden usar, distribuir y modificar sus obras.'], label=1.0),
    InputExample(texts=['Plan de Gestión de Datos (PGD)', 'Un PGD es un documento formal que describe cómo se gestionarán los datos de investigación durante y después de un proyecto, incluyendo su recopilación, organización, almacenamiento, acceso y preservación.'], label=1.0),
    InputExample(texts=['Identificadores persistentes (PIDs)', 'Los PIDs son identificadores únicos y permanentes utilizados para referenciar objetos digitales (como datasets, publicaciones o investigadores) de manera consistente a lo largo del tiempo, facilitando su descubrimiento y citación.'], label=1.0),
    InputExample(texts=['Software de código abierto en investigación', 'El software de código abierto es fundamental para la reproducibilidad y transparencia en la investigación, permitiendo a otros usuarios acceder, modificar y ejecutar el código utilizado en un estudio.'], label=1.0),
    InputExample(texts=['Métricas alternativas (Altmetrics)', 'Las altmetrics son métricas complementarias a las citas tradicionales que miden el impacto de la investigación en línea, como menciones en redes sociales, blogs, noticias o repositorios.'], label=1.0),
    InputExample(texts=['¿Cuál es la diferencia entre acceso abierto y datos abiertos?', 'El acceso abierto se centra principalmente en que las publicaciones de investigación estén libremente disponibles, mientras que los datos abiertos se refieren a hacer que los datos subyacentes a la investigación sean accesibles y reutilizables.'], label=0.9),
    InputExample(texts=['Repositorio de datos vs Repositorio de publicaciones', 'Un repositorio de datos almacena datasets de investigación, mientras que un repositorio de publicaciones almacena artículos, preprints u otros textos académicos.'], label=0.9),
    InputExample(texts=['Peer review tradicional vs evaluación abierta', 'La revisión por pares tradicional suele ser ciega o doblemente ciega, mientras que la evaluación abierta busca mayor transparencia en el proceso, a veces revelando la identidad de los revisores y los comentarios.'], label=0.9),
    InputExample(texts=['Licencia CC BY vs CC BY-SA', 'Ambas son licencias Creative Commons que requieren atribución, pero CC BY-SA (ShareAlike) además exige que las obras derivadas se distribuyan bajo la misma licencia.'], label=0.9),
    InputExample(texts=['¿Qué significa reproducible?', 'La reproducibilidad en ciencia implica que un investigador, usando los mismos datos y código que el investigador original, puede obtener los mismos resultados.'], label=1.0),
    InputExample(texts=['Plataforma de ciencia ciudadana', 'Ejemplos de plataformas incluyen Zooniverse, donde los voluntarios ayudan a analizar grandes conjuntos de datos.'], label=0.8),
    InputExample(texts=['Identificador ORCID', 'ORCID es un identificador persistente para investigadores, ayudando a distinguirlos y vincular su producción científica.'], label=0.8),
    InputExample(texts=['Preprint', 'Un preprint es una versión de un artículo de investigación que se publica antes de ser revisado por pares en una revista formal.'], label=1.0),
    InputExample(texts=['Ciencia abierta en América Latina', 'La ciencia abierta en América Latina enfrenta desafíos y oportunidades únicas relacionadas con la infraestructura, las políticas y la cultura académica regional.'], label=1.0),
    InputExample(texts=['¿Cómo define la UNESCO la ciencia abierta?', 'La ciencia abierta se define como un constructo inclusivo que combina diversos movimientos y prácticas con el fin de que los conocimientos científicos multilingües estén abiertamente disponibles y sean accesibles para todos, así como reutilizables por todos, se incrementen las colaboraciones científicas y el intercambio de información en beneficio de la ciencia y la sociedad, y se abran los procesos de creación, evaluación y comunicación de los conocimientos científicos a los agentes sociales más allá de la comunidad científica tradicional.'], label=1.0),
    InputExample(texts=['¿Cuáles son los pilares fundamentales de la ciencia abierta según la UNESCO?', 'La ciencia abierta se basa en los siguientes pilares clave: conocimiento científico abierto, infraestructuras de la ciencia abierta, comunicación científica, participación abierta de los agentes sociales y diálogo abierto con otros sistemas de conocimiento.'], label=1.0)
]

# Crear conjunto de evaluación (usando una porción de los datos de ejemplo) Utilizo 70/30
# En un escenario real, deberías tener un conjunto de evaluación separado
eval_examples = examples[-6:]  # Últimos 6 ejemplos para evaluación, es decir utilizo casi el 30% del conjunto de ejemplos
train_examples = examples[:-6]  # Resto para entrenamiento, es decir, el 70 %.

def evaluate_model_metrics(model, eval_examples, threshold=0.8):
    """
    Evalúa el modelo calculando métricas de clasificación basadas en similitud coseno
    """
    print("\n" + "="*60)
    print("EVALUACIÓN DE MÉTRICAS DEL MODELO")
    print("="*60)

    true_labels = []
    predicted_labels = []
    similarities = []

    for example in eval_examples:
        # Obtener embeddings de ambos textos
        embedding1 = model.encode([example.texts[0]])
        embedding2 = model.encode([example.texts[1]])

        # Calcular similitud coseno
        similarity = cosine_similarity(embedding1, embedding2)[0][0]
        similarities.append(similarity)

        # Convertir a etiqueta binaria usando threshold
        predicted_label = 1 if similarity >= threshold else 0
        true_label = 1 if example.label >= threshold else 0

        predicted_labels.append(predicted_label)
        true_labels.append(true_label)

        print(f"Texto 1: {example.texts[0][:50]}...")
        print(f"Texto 2: {example.texts[1][:50]}...")
        print(f"Similitud real: {example.label:.3f} | Similitud predicha: {similarity:.3f}")
        print(f"Etiqueta real: {true_label} | Etiqueta predicha: {predicted_label}")
        print("-" * 40)

    # Calcular métricas
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision, recall, f1, support = precision_recall_fscore_support(
        true_labels, predicted_labels, average='binary', zero_division=0
    )

    # Mostrar métricas principales
    print(f"\n📊 MÉTRICAS DE RENDIMIENTO:")
    print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
    print(f"Recall:    {recall:.4f} ({recall*100:.2f}%)")
    print(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)")

    # Estadísticas de similitud
    similarities = np.array(similarities)
    print(f"\n📈 ESTADÍSTICAS DE SIMILITUD:")
    print(f"Similitud promedio:    {np.mean(similarities):.4f}")
    print(f"Desviación estándar:   {np.std(similarities):.4f}")
    print(f"Similitud mínima:      {np.min(similarities):.4f}")
    print(f"Similitud máxima:      {np.max(similarities):.4f}")

    # Reporte de clasificación detallado
    print(f"\n📋 REPORTE DE CLASIFICACIÓN DETALLADO:")
    print(classification_report(true_labels, predicted_labels,
                              target_names=['Baja similitud', 'Alta similitud'],
                              zero_division=0))

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'similarities': similarities,
        'true_labels': true_labels,
        'predicted_labels': predicted_labels
    }

def compare_models_performance(original_model, fine_tuned_model, eval_examples):
    """
    Compara el rendimiento entre el modelo original y el fine-tuned
    """
    print("\n" + "="*60)
    print("COMPARACIÓN DE MODELOS")
    print("="*60)

    print("Evaluando modelo ORIGINAL...")
    original_metrics = evaluate_model_metrics(original_model, eval_examples)

    print("\nEvaluando modelo FINE-TUNED...")
    finetuned_metrics = evaluate_model_metrics(fine_tuned_model, eval_examples)

    # Crear tabla comparativa
    comparison_data = {
        'Métrica': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Similitud Promedio'],
        'Modelo Original': [
            f"{original_metrics['accuracy']:.4f}",
            f"{original_metrics['precision']:.4f}",
            f"{original_metrics['recall']:.4f}",
            f"{original_metrics['f1_score']:.4f}",
            f"{np.mean(original_metrics['similarities']):.4f}"
        ],
        'Modelo Fine-tuned': [
            f"{finetuned_metrics['accuracy']:.4f}",
            f"{finetuned_metrics['precision']:.4f}",
            f"{finetuned_metrics['recall']:.4f}",
            f"{finetuned_metrics['f1_score']:.4f}",
            f"{np.mean(finetuned_metrics['similarities']):.4f}"
        ]
    }

    df_comparison = pd.DataFrame(comparison_data)
    print("\n📊 TABLA COMPARATIVA:")
    print(df_comparison.to_string(index=False))

    # Calcular mejoras
    accuracy_improvement = finetuned_metrics['accuracy'] - original_metrics['accuracy']
    f1_improvement = finetuned_metrics['f1_score'] - original_metrics['f1_score']

    print(f"\n🚀 MEJORAS OBTENIDAS:")
    print(f"Mejora en Accuracy: {accuracy_improvement:+.4f} ({accuracy_improvement*100:+.2f}%)")
    print(f"Mejora en F1-Score: {f1_improvement:+.4f} ({f1_improvement*100:+.2f}%)")

# Cargar el modelo original para comparación
print("Cargando modelo original...")
original_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

# Evaluar modelo original ANTES del fine-tuning
print("Evaluando modelo ANTES del fine-tuning...")
original_metrics = evaluate_model_metrics(original_model, eval_examples)

# Load the model para fine-tuning
modelFT = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

# Create a DataLoader
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# Define the loss function
train_loss = losses.CosineSimilarityLoss(model=modelFT)

# Define number of epochs
num_epochs = 2

# Crear evaluador para seguimiento durante el entrenamiento
evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(
    eval_examples, name='eval'
)

# Fine-tuning con evaluación
print(f"\n🔄 Iniciando fine-tuning del modelo de embeddings por {num_epochs} época(s)...")
print("="*60)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=num_epochs,
    warmup_steps=100,
    evaluator=evaluator,
    evaluation_steps=50,  # Evaluar cada 50 pasos
    output_path="./fine_tuned_model"
)

print("\n✅ Fine-tuning completado!")

# Evaluar modelo DESPUÉS del fine-tuning
print("\nEvaluando modelo DESPUÉS del fine-tuning...")
finetuned_metrics = evaluate_model_metrics(modelFT, eval_examples)

# Comparar rendimiento
compare_models_performance(original_model, modelFT, eval_examples)

# Guardar el modelo
modelFT.save("fine_tuned_model")
print(f"\n💾 Modelo guardado en: ./fine_tuned_model")

# Función adicional para evaluar en nuevos ejemplos
def test_model_on_new_examples(model, new_examples):
    """
    Función para probar el modelo en nuevos ejemplos
    """
    print("\n" + "="*60)
    print("PRUEBA CON NUEVOS EJEMPLOS")
    print("="*60)

    for i, (text1, text2) in enumerate(new_examples, 1):
        embedding1 = modelFT.encode([text1])
        embedding2 = modelFT.encode([text2])
        similarity = cosine_similarity(embedding1, embedding2)[0][0]

        print(f"Ejemplo {i}:")
        print(f"Texto 1: {text1}")
        print(f"Texto 2: {text2}")
        print(f"Similitud: {similarity:.4f}")
        print("-" * 40)

# Ejemplo de uso con nuevos textos
nuevos_ejemplos = [
    ("¿Qué son los datos abiertos?", "Los datos abiertos son datos que pueden ser utilizados, reutilizados y redistribuidos libremente por cualquier persona."),
    ("Política de ciencia abierta", "Las políticas institucionales promueven prácticas de acceso abierto en las universidades."),
    ("Gatos y perros", "Los datos FAIR son importantes para la investigación reproducible.")
]

print(f"\nProbando modelo fine-tuned con nuevos ejemplos...")
test_model_on_new_examples(modelFT, nuevos_ejemplos)