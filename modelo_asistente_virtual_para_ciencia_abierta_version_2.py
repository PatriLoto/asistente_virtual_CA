# -*- coding: utf-8 -*-
"""modelo_asistente_virtual_para_ciencia_abierta_version_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gbb71O5nbnWOL0PiklTR5Gt5BYbrEJtA

# 1. Librer√≠as utilizadas.
"""

# # Lectura e ingesta de PDFs
# os
# pdfplumber==0.11.7

# # #Limpieza y Procesamiento de documentos
# # spacy==3.8.7
# nltk==3.9.1
# pandas==2.2.2
# regex==2024.11.6


# # Embeddings y vectorizaci√≥n
# sentence-transformers==4.1.0
# chromadb==1.0.15    (base de datos vectorial)
# faiss-cpu ==1.7.4

# # Framework RAG (Seleccionar langchain o llama-index no es necesario utilizar las dos)
# langchain==0.3.27
# langchain-community==0.3.27
# langchain-core==0.3.72
# langchain-text-splitters==0.3.9
# rank-bm25==0.2.2
# llama-index ==0.9.0.

# # APIs LLM (Seleccionar openai o anthropic de claude no es necesario utilizar las dos)
# huggingface-hub==0.34.3
# openai==1.98.0
# transformers==4.54.1
# torch @ https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl

# # Interfaz y API (Para m√°s adelante, elegir gradio o streamlit, no es necesario utilizar las dos)
# streamlit==1.28.0
# gradio==4.8.0  (utilizar esta por su sencillez, luego en la iteraci√≥n agregar streamlit)

"""## 1.1 Instalaci√≥n de librer√≠as"""

!pip install pdfplumber spacy sentence_transformers -U langchain-community chromadb rank_bm25
!pip install -U :class:`~langchain-huggingface.HuggingFaceEmbeddings

"""## 1.2 Importaci√≥n de librer√≠as"""

# Librer√≠as principales de python
import pdfplumber # Extracci√≥n de texto PDF
import spacy
import sentence_transformers  # Embeddings sem√°nticos
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path
# import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2") # Use model_name directly

"""# 2. Pipeline de Procesamiento de Documentos

## 2.1 Ingesta y preprocesamiento de los archivos PDFs
"""

import os
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() + "\n"
    except Exception as e:
        print(f"Error extracting text from {pdf_path}: {e}")
    return text

def split_text_into_chunks(text, chunk_size=500, chunk_overlap=100):
    """Splits text into smaller chunks."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    chunks = text_splitter.split_text(text)
    return chunks

# Define the path to your PDF folder
input_folder = "pdfs"
folder_path = Path(input_folder)

# Check if the folder exists
if not folder_path.exists():
    print(f"Error: Folder '{input_folder}' not found.")
    # Create the folder if it doesn't exist and add a placeholder file
    try:
        folder_path.mkdir(parents=True, exist_ok=True)
        print(f"Carpeta creada: '{input_folder}'. Por favor sub√≠ tus archivos pdfs aqu√≠.")
        # # Create a placeholder file to indicate where to put PDFs
        # placeholder_file = folder_path / "upload_your_pdfs_here"
        # with open(placeholder_file, "w") as f:
        #     f.write("Por favor sub√≠ tus archivos pdfs a esta carpeta.")
    except Exception as e:
        print(f"Error creating folder '{input_folder}': {e}")
    pdf_files = [] # Set pdf_files to empty list if folder doesn't exist
else:
    # Get a list of all PDF files in the folder
    pdf_files = list(folder_path.glob("*.pdf"))

all_document_text = ""
if pdf_files:
    print(f" {len(pdf_files)} PDF files in '{input_folder}'. Processing...")
    for pdf_file in pdf_files:
        print(f"Processing: {pdf_file}")
        document_text = extract_text_from_pdf(pdf_file)
        if document_text:
            all_document_text += document_text + "\n"
    print("Finished processing PDF files.")

    # Split the combined text into chunks
    if all_document_text:
        text_chunks = split_text_into_chunks(all_document_text)
        print(f"Successfully extracted and split {len(text_chunks)} chunks from all documents.")
    else:
        print("No text extracted from any PDF files.")
        text_chunks = []

else:
    print(f"No PDF files found in '{input_folder}'. Please upload your PDF files to this folder.")
    text_chunks = [] # Initialize as empty list if no files are found

# Now text_chunks contains all chunks from all processed PDFs

"""## 2.2 Procesamiento de los archivos pdfs
Anteriormente los archivos pdfs fueron segmentados en chunks de 500 a 1000 tokens, en el siguiente chunk se procede a la limpieza y normalizaci√≥n del texto en espa√±ol.

## 2.2.1 Descarga de recursos necesarios de Spacy
"""

# Cargar modelo de spaCy para espa√±ol
try:
   nlp = spacy.load("es_core_news_sm")
   print("Modelo de spaCy cargado.")
except OSError:
  print("Modelo de spaCy no encontrado. Se instalar√° a continuaci√≥n.")
  !python -m spacy download es_core_news_sm
  nlp = spacy.load("es_core_news_sm")
  #  print("Modelo de spaCy no encontrado. Instalar con: python -m spacy download es_core_news_sm")
  #  nlp = None

"""## 2.2.2 Limpieza del texto"""

import re
import spacy
from typing import List, Dict, Tuple
import pandas as pd
from collections import defaultdict


class DocumentProcessor:
   def __init__(self, chunk_size_tokens=500, chunk_overlap_tokens=100):
       self.chunk_size_tokens = chunk_size_tokens
       self.chunk_overlap_tokens = chunk_overlap_tokens
       self.nlp = nlp


   def clean_and_normalize_spanish(self, text: str) -> str:  # podr√≠a agregar algunas reglas de limpieza del  C√≥digo del CORPUS
       """
       Limpieza y normalizaci√≥n de texto en espa√±ol
       """
       # Normalizar caracteres especiales del espa√±ol
       replacements = {
           '√°': '√°', '√©': '√©', '√≠': '√≠', '√≥': '√≥', '√∫': '√∫',
           '√±': '√±', '√º': '√º', '√Å': '√Å', '√â': '√â', '√ç': '√ç',
           '√ì': '√ì', '√ö': '√ö', '√ë': '√ë', '√ú': '√ú'
       }

       # Normalizar espacios y saltos de l√≠nea
       text = re.sub(r'\s+', ' ', text)
       text = re.sub(r'\n+', '\n', text)

       # Eliminar caracteres de control
       text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)

       # Normalizar puntuaci√≥n
       text = re.sub(r'["""]', '"', text)
       text = re.sub(r"['']", "'", text) # Fixed the regex pattern for single quotes
       text = re.sub(r'[‚Äì‚Äî]', '-', text)

       # Eliminar espacios antes de puntuaci√≥n
       text = re.sub(r'\s+([,.;:!?])', r'\1', text)

       # Normalizar n√∫meros y fechas
       text = re.sub(r'(\d+)\s*,\s*(\d+)', r'\1,\2', text)
       text = re.sub(r'(\d+)\s*\.\s*(\d+)', r'\1.\2', text)

       # Eliminar URLs y emails (preservar contexto)
       text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-Z]))+', '[URL]', text)
       text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
       text = re.sub(r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-Z]))+', '', text)

       # Limpiar espacios m√∫ltiples
       text = re.sub(r'\s+', ' ', text)

      #---------------------------------------------
      # LIMPIEZA AGREGADA DESDE EL CODIGO DE CORPUS
      #---------------------------------------------

       # Eliminar n√∫meros de p√°gina (patrones comunes)
      # Elimina n√∫meros solos en una l√≠nea o n√∫meros con formato "P√°gina X"
       text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)
       text = re.sub(r'p√°gina\s*\d+', '', text, flags=re.IGNORECASE)
       text = re.sub(r'pp' '\s*\d+', '', text, flags=re.IGNORECASE)
       text = re.sub(r'p\.\s*\d+', '', text, flags=re.IGNORECASE)
       text = re.sub(r'p√°g\.\s*\d+', '', text, flags=re.IGNORECASE)
       text = re.sub(r'^-\s*\d+\s*-\s*$', '', text, flags=re.MULTILINE)

        # Tambi√©n manejar casos donde hay espacios antes del salto de l√≠nea
        # Como "nac- ieron" en la misma l√≠nea
       text = re.sub(r'(\w+)-\s+(\w+)', r'\1\2', text)

        # Eliminar patrones tipo peerj.preprints.27580v1 o similares
        # Este patr√≥n captura: palabra.palabra.n√∫mero+letra+n√∫mero
       text = re.sub(r'\b\w+\.\w+\.\d+[a-zA-Z]+\d*\b', ' ', text)

        # Eliminar patrones de identificadores de documentos cient√≠ficos
        # Como: arxiv.1234.5678, doi.10.1234/5678, pmid.12345678
       text = re.sub(r'\b\w+\.\d+\.\d+\b', ' ', text)
       text = re.sub(r'\bdoi\.\borg\.\S+', ' ', text)
       text = re.sub(r'\bpmid\.\d+', ' ', text)
       text = re.sub(r'\barxiv\.\d+\.\d+', ' ', text)


        # Eliminar n√∫meros con comas como 45, 56, 78
        # Este patr√≥n elimina secuencias de n√∫meros separados por comas
       text = re.sub(r'\b\d+(?:\s*,\s*\d+)+\b', '', text)

        # Eliminar n√∫meros solos (incluyendo decimales)
        # Esto elimina n√∫meros aislados como 123, 45.67, etc.
       text = re.sub(r'\b\d+\.?\d*\b', '', text)

        # Eliminar referencias tipo [1], [2,3], [45-47], etc.
       text = re.sub(r'\[\d+(?:[-,]\d+)*\]', '', text)

        # Eliminar a√±os solos (4 d√≠gitos)
       text = re.sub(r'\b\d{4}\b', '', text)

        # casos especiales de limpieza del texto de UNESCO
        #Eliminar n√∫meros romanos en min√∫scula
        #roman_numerals_pattern = r'\b(?:i[vx]|v?i{0,3}|x)\b'
        # elimina numeracion del tipo 45¬™
       text = re.sub(r'\b\d+¬™\b', ' ', text)
        #elimina url que solo terminan con '.com'
       text = re.sub(r'\b(?!https:\/\/|www\.)[a-zA-Z0-9.-]+\.com\b', ' ', text)


        # casos especiales de limpieza del texto de Conocimiento abierto
        # elimina letras solas
       text = re.sub(r'\b[a-z]\b', ' ', text)
        # elimina el siguiente patr√≥n: ccoolleeccccii√≥√≥nn ggrruuppooss ddee ttrraabbaajjoo ccoonnoocciimmiieennttoo aabbiieerrttoo eenn aamm√©√©rriiccaa llaattiinnaa
       text = re.sub(r'(?<!acc)(?<!desarroll)([a-z√°√©√≠√≥√∫√±])\1+(?!eso)(?!lo)',r'\1', text) # ver si conviene o no, tiene palabras en ingles como access(queda aces) o common(queda comon)
        # filtra las palabras de 2 digitos: Ej:fs, ar, eb
       text = re.sub(r'\b[a-z]{2}\b', ' ', text)
        # filtra las palabras de 3 digitos sin vocales: Ej:pcb
       text = re.sub(r'\b[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]{3}\b', ' ', text)
        # filtra las asociaciones de este tipo: n2o0s2, a0i
       text = re.sub(r'\b(?=\w*\d)(?=\d*\w)\w+\b', ' ', text)

        # Remove HTML tags
       text = re.sub ( r'<.*?>' ,'', text)

        # Eliminar correos electr√≥nicos
       text = re.sub(r'\S+@\S+', '', text)

        # Eliminar caracteres especiales excesivos pero mantener puntuaci√≥n b√°sica
        # Mantener: . , ; : ! ? ¬ø ¬° ( ) - " '
        # text = re.sub(r'[^\w\s\.\,\;\:\!\?\¬ø\¬°\(\)\-\"\¬™\'√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', '', text)
        # decido eliminar todos los caracteres especiales por eso comento lo anterior
       text = re.sub(r'[^\w\s]', '', text)

        # Eliminar m√∫ltiples espacios
       text = re.sub(r'\s+', ' ', text)

        # Eliminar m√∫ltiples saltos de l√≠nea
       text = re.sub(r'\n+', '\n', text)

        # Eliminar l√≠neas que solo contienen espacios
       lines = text.split('\n')
       lines = [line.strip() for line in lines if line.strip()]
       text = '\n'.join(lines)

       # Eliminar stopwords de spaCy
       if self.nlp:
            doc = self.nlp(text)
            text = " ".join([token.text for token in doc if not token.is_stop])
            print(f"Successfully cleaned and normalized {len(cleaned_text_chunks)} text chunks.")

       return text.strip()

"""# 3. Arquitectura de Retrieval

## 3.1 Sistema de B√∫squeda Multimodal

En sintesis, este c√≥digo prepara los datos de texto para su recuperaci√≥n convirti√©ndolos a un formato (documentos con sus correspondientes embeddings almacenados en una base de datos vectorial) que permite buscar de manera eficiente informaci√≥n relevante basada en la similitud sem√°ntica. Este es un paso fundamental en un proceso RAG, ya que permite encontrar las partes m√°s relevantes de los documentos para responder a las consultas de los usuarios. Los pasos que se utilizaron son:

* creaci√≥n de objetos del tipo Document a partir de los fragmentos de texto.
* creaci√≥n de un vector_store (utilizando Chroma) a partir de los objetos Document y sus correspondientes embeddings. Por √∫ltimo, se utilizar√°n los documentos y el vector_store para configurar el BM25Retriever, VectorStoreRetriever y EnsembleRetriever.
"""

from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# Create Document objects from text_chunks
if text_chunks is not None:
    documents = [Document(page_content=chunk) for chunk in text_chunks]
    print(f"Created {len(documents)} Document objects from cleaned chunks.")
else:
    documents = []
    print("No cleaned text chunks available to create documents.")

# Assuming you have already loaded the model
# If not, you need to run the cell that loads the model (cell id: 4417745d)
if documents and model is not None: # Check if documents and the model are available
    # Create a HuggingFaceEmbeddings object using your SentenceTransformer model
    # Aplico embeddings del texto de los pdfs
    embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2") # Use model_name directly

    # Create a Chroma vector store, passing the embedding function
    vector_store = Chroma.from_documents(documents, embedding_function)
    print("Chroma vector store created from documents.")
else:
    vector_store = None
    print("Could not create Chroma vector store: no documents or embedding model available.")

#para aplicar lo siguiente antes debo cargar los objetos de tipo Documents, lo que se realiza en el paso anterior
from langchain.vectorstores import Chroma
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# Retriever h√≠brido
bm25_retriever = BM25Retriever.from_documents(documents)
vector_retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.7, "k": 5}
)

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.3, 0.7]
)

"""# 4. Integraci√≥n con APIs de Grandes Modelos de Lenguaje

La clase LLMProvider permite cambiar f√°cilmente entre diferentes proveedores de un gran modelo del lenguaje, simplemente especificando el nombre del proveedor al crear una instancia de la clase, sin tener que modificar el c√≥digo que utiliza el cliente. Esto facilita la experimentaci√≥n con diferentes modelos.

## 4.1 Prompt Engineering Especializado
"""

SYSTEM_PROMPT = """
Eres un asistente especializado en Ciencia Abierta para investigadores y estudiantes de posgrado de Am√©rica Latina.

Tu conocimiento se basa en:
- Recomendaci√≥n UNESCO sobre Ciencia Abierta (2021)
- Recursos de Redalyc
- Documentos especializados en espa√±ol
- Contexto regional latinoamericano
- Mejores pr√°cticas acad√©micas utilizadas en Am√©rica Latina

Responde de manera:
- Precisa y basada en evidencia
- Adaptada al nivel del usuario: b√°sico, intermedio, avanzado
- Con ejemplos del contexto latinoamericano
- Incluyendo fuentes y recursos adicionales

Contexto recuperado: {context}
Perfil del usuario: {user_profile}
"""

"""## 4.2 Integraci√≥n y Evaluaci√≥n de Modelos LLM

En el contexto del sistema RAG utilizado en esta secci√≥n:

* La clase LLMProvider es la encargada de la fase de "Generation".
Esta recibe la pregunta del usuario y el contexto relevante (obtenido de la fase de "Retrieval" que ya implementaste previamente con los EnsembleRetriever, BM25Retriever y VectorStoreRetriever).
* Luego, utiliza un ***system_promp***t especializado para guiar al LLM a responder como un experto en Ciencia Abierta, considerando el contexto recuperado.
* Por √∫ltimo, la clase LLMEvaluator te ayuda a medir la calidad de las respuestas generadas, permiti√©ndote iterar y mejorar tu sistema RAG
"""

import os
import time
import json
from openai import OpenAI
from anthropic import Anthropic
# Import the new MistralClient from the updated library
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage # Import ChatMessage if needed for future use
from google.colab import userdata
# from dotenv import load_dotenv

# Carga las variables de entorno desde un archivo .env
# load_dotenv()   # Y EN LA SECCION DE LECTURA VA: return OpenAI(api_key=userdata.getload("OPENAI_API_KEY"))

# --- Este es el trozo de c√≥digo que proporcionaste ---
class LLMProvider:
    def __init__(self, provider="mistral"):         # por defecto, est√° configurado con OpenAi pero lo cambio a Mistral por el l√≠mite que tiene el primer proveedor
    #def __init__(self, provider="openai"):
        self.provider = provider
        # Se asume que las API keys est√°n en las secrets, a futuro se debe pasar estos valores a variables del entorno
        # KEY_OPENAI_API, ANTHROPIC_API_KEY, HUGGINGFACEHUB_API_TOKEN
        self.client = self._initialize_client()

    def _initialize_client(self):
        if self.provider == "openai":
            # Use userdata.get() to access the API key from Colab Secrets
            return OpenAI(api_key=userdata.get("OPENAI_API_KEY"))
        elif self.provider == "mistral":
            # Updated initialization for MistralClient
           return MistralClient(api_key=userdata.get("MISTRAL_API_KEY"))
        elif self.provider == "anthropic":
            return Anthropic(api_key=userdata.get("ANTHROPIC_API_KEY"))
        elif self.provider == "huggingface":
            # Esta implementaci√≥n puede variar seg√∫n la librer√≠a exacta que uses
            # Aqu√≠ se asume un cliente compatible con un m√©todo 'invoke' o similar
            print("Hugging Face client initialization needs specific implementation.")
            return HuggingFaceHub(repo_id="microsoft/DialoGPT-large")

            # return HuggingFaceHub(repo_id="google/flan-t5-xxl", ...)
            return None
        else:
            raise ValueError("Proveedor no soportado")

    # --- INICIO DEL C√ìDIGO A√ëADIDO ---

    def generate_response(self, prompt, model="mistral-tiny", max_tokens=1500, temperature=0.4):           # como est√° configurado con OpenAi, se utiliza el modelo "gpt-4o". Si queremos otro modelo debemos revisar el proveedor, es decir, saber si es Mistral, huggingFace, etc.
        """
        Genera una respuesta utilizando el cliente del proveedor LLM configurado.

        Args:
            prompt (str): El prompt del usuario.
            model (str): El modelo a utilizar (relevante para OpenAI/Anthropic/Mistral).
            max_tokens (int): El n√∫mero m√°ximo de tokens en la respuesta.
            temperature (float): La "creatividad" de la respuesta.

        Returns:
            dict: Un diccionario con la 'response' y la 'latency' en segundos.
        """
        # system_prompt = "Eres un asistente experto en Ciencia Abierta. Tu prop√≥sito es proporcionar respuestas claras, precisas y bien estructuradas sobre sus principios, pr√°cticas y beneficios."
          system_prompt = """
          Eres un asistente especializado en Ciencia Abierta para investigadores y estudiantes de posgrado de Am√©rica Latina.

          Tu conocimiento se basa en:
          - Recomendaci√≥n UNESCO sobre Ciencia Abierta (2021)
          - Recursos de Redalyc
          - Documentos especializados en espa√±ol
          - Contexto regional latinoamericano
          - Mejores pr√°cticas acad√©micas utilizadas en Am√©rica Latina

          Responde de manera:
          - Precisa y basada en evidencia
          - Adaptada al nivel del usuario: b√°sico, intermedio, avanzado
          - Con ejemplos del contexto latinoamericano
          - Incluyendo fuentes y recursos adicionales

          Contexto recuperado: {context}
          Perfil del usuario: {user_profile}
          """
        start_time = time.time()

        try:
            response_text = ""
            if self.provider == "openai":
                completion = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                response_text = completion.choices[0].message.content

            elif self.provider == "anthropic":
                message = self.client.messages.create(
                    model= "claude-3-sonnet", # por ejemplo model="claude-3-opus-20240229", este √∫ltimo es m√°s avanzado pero de pago
                    max_tokens=max_tokens,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                response_text = message.content[0].text

            elif self.provider == "mistral":
                 messages = [ChatMessage(role="user", content=prompt)] # Use ChatMessage for the new client
                 chat_response = self.client.chat(
                     model=model,                 # por ejemplo model= model  "mistral-tiny"  # Tambi√©n puedes usar "mistral-small" o "mistral-medium"
                     messages=messages,
                     max_tokens=max_tokens,
                     temperature=temperature
                 )
                 response_text = chat_response.choices[0].message.content


            # elif self.provider == "huggingface":
            #     response_text = self.client.invoke(prompt)

            else:
                raise ValueError(f"Proveedor '{self.provider}' no implementado para generaci√≥n.")

            end_time = time.time()
            latency = end_time - start_time

            return {"response": response_text, "latency": latency}

        except Exception as e:
            print(f"Error al generar respuesta con {self.provider}: {e}")
            return {"response": None, "latency": 0}


class LLMEvaluator:
    """
    Eval√∫a la calidad de una respuesta de un LLM utilizando otro LLM (LLM-as-a-judge).
    """
    def __init__(self, evaluator_provider="openai", evaluator_model="gpt-3.5-turbo"):
        """
        Inicializa el evaluador con un LLM espec√≠fico que actuar√° como juez.
        """
        print(f"Inicializando evaluador con {evaluator_provider} ({evaluator_model})...")
        # El evaluador usa su propia instancia de LLMProvider para ser el "juez"
        self.judge_llm = LLMProvider(provider=evaluator_provider)
        self.evaluator_model = evaluator_model

    def evaluate(self, prompt, response, latency):
        """
        Eval√∫a una respuesta bas√°ndose en criterios de calidad y rendimiento.

        Args:
            prompt (str): El prompt original.
            response (str): La respuesta generada.
            latency (float): La latencia de la generaci√≥n.

        Returns:
            dict: Un diccionario con las m√©tricas de rendimiento y calidad.
        """
        if not response:
            return {"error": "No se proporcion√≥ una respuesta para evaluar."}

        evaluation_prompt = f"""
        Eres un sistema de evaluaci√≥n experto. Tu tarea es analizar la calidad de una respuesta generada por un modelo de lenguaje sobre el tema de "Ciencia Abierta".

        Eval√∫a la respuesta bas√°ndose en los siguientes criterios, asignando una puntuaci√≥n del 1 (muy pobre) al 5 (excelente) para cada uno:
        1.  **Relevancia**: ¬øLa respuesta aborda directamente la pregunta del prompt?
        2.  **Precisi√≥n**: ¬øLa informaci√≥n proporcionada es correcta y fiable?
        3.  **Coherencia y Claridad**: ¬øLa respuesta est√° bien estructurada, es f√°cil de entender y no tiene contradicciones?

        PROMPT ORIGINAL:
        "{prompt}"

        RESPUESTA GENERADA:
        "{response}"

        Por favor, devuelve tu evaluaci√≥n √∫nicamente en formato JSON, con la siguiente estructura:
        {{
            "puntuacion_relevancia": <int>,
            "puntuacion_precision": <int>,
            "puntuacion_coherencia": <int>,
            "razonamiento_evaluacion": "<string con tu justificaci√≥n>"
        }}
        """
        print("\n--- Solicitando evaluaci√≥n de calidad al LLM juez... ---")
        # Usamos el generador del juez para obtener la evaluaci√≥n
        evaluation_result = self.judge_llm.generate_response(
            prompt=evaluation_prompt,
            model=self.evaluator_model
        )

        try:
            # Parseamos el JSON de la evaluaci√≥n, handling potential non-JSON response
            quality_metrics = json.loads(evaluation_result["response"])
        except (json.JSONDecodeError, TypeError):
            print("Error: La evaluaci√≥n del juez no devolvi√≥ un JSON v√°lido.")
            # Provide the raw response if JSON parsing fails
            quality_metrics = {"error": "Fallo al parsear la evaluaci√≥n.", "raw_output": evaluation_result.get("response", "No response received.")}


        # Combinamos todas las m√©tricas en un resultado final
        final_report = {
            "metricas_rendimiento": {
                "latencia_segundos": round(latency, 2)
            },
            "metricas_calidad": quality_metrics
        }

        return final_report


# --- Ejemplo de uso ---
if __name__ == "__main__":
    # 1. Elige el proveedor para generar la respuesta principal
    # Puedes cambiar "openai" por "anthropic" si tienes la API key configurada
    provider_principal = LLMProvider(provider="mistral")

    # 2. Define tu prompt sobre Ciencia Abierta
    prompt_usuario = "¬øCu√°les son los pilares fundamentales de la Ciencia Abierta y c√≥mo benefician a la investigaci√≥n colaborativa?"

    print(f"Generando respuesta con '{provider_principal.provider}'...")
    print(f"Prompt: {prompt_usuario}\n")

    # 3. Genera la respuesta y obt√©n la latencia
    # Para Anthropic, podr√≠as usar un modelo como "claude-3-sonnet-20240229", este es pago
    resultado_generacion = provider_principal.generate_response(
        prompt_usuario,
        model="mistral-tiny" # Changed model to a Mistral model as the provider is set to mistral
    )

    if resultado_generacion and resultado_generacion["response"]:
        respuesta_generada = resultado_generacion["response"]
        latencia_generacion = resultado_generacion["latency"]

        print("‚úÖ Respuesta Generada:")
        print("--------------------")
        print(respuesta_generada)
        print("--------------------")
        print(f"‚è±Ô∏è Tiempo de generaci√≥n: {latencia_generacion:.2f} segundos")

        # 4. Eval√∫a la respuesta generada
        # Se puede usar el mismo proveedor (ej. OpenAI con GPT-4o) como juez por su alta capacidad de razonamiento
        evaluador = LLMEvaluator(evaluator_provider="mistral", evaluator_model="gpt-4o")   # buscar configurar con Antropic q es el pago q tengo  evaluador = LLMEvaluator(evaluator_provider="openai", evaluator_model="gpt-4o")
        reporte_final = evaluador.evaluate(prompt_usuario, respuesta_generada, latencia_generacion)

        # 5. Muestra el reporte de evaluaci√≥n
        print("\n\nüìä Reporte de Evaluaci√≥n:")
        print("========================")
        # Imprime el reporte final de una forma legible
        print(json.dumps(reporte_final, indent=4, ensure_ascii=False))

"""# 5. EXTRA: Fine-tuning para evaluar el modelo de Embeddings

Este c√≥digo est√° dise√±ado para fine-tune (ajustar) un modelo de embeddings pre-entrenado (sentence-transformers/paraphrase-multilingual-mpnet-base-v2) utilizando ejemplos espec√≠ficos de tu dominio (Ciencia Abierta en espa√±ol). El objetivo es mejorar la capacidad del modelo para generar embeddings que capturen mejor la similitud sem√°ntica entre textos relacionados con este tema.
"""

from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation
from torch.utils.data import DataLoader
import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Ejemplos de dominio-espec√≠fico
examples = [
    InputExample(texts=['¬øQu√© es el acceso abierto?', 'El acceso abierto es una pr√°ctica que permite a los usuarios leer, descargar, copiar, distribuir, imprimir, buscar o enlazar los textos completos de art√≠culos cient√≠ficos y de investigaci√≥n y utilizarlos para cualquier otro fin leg√≠timo, sin barreras financieras, legales o t√©cnicas, con la √∫nica salvedad de las que supone Internet misma.'], label=1.0),
    InputExample(texts=['Datos FAIR', 'Los principios FAIR (Findable, Accessible, Interoperable, Reusable) son un conjunto de directrices para la gesti√≥n y administraci√≥n de datos de investigaci√≥n, asegurando que los datos sean localizables, accesibles, interoperables y reutilizables tanto por humanos como por m√°quinas.'], label=1.0),
    InputExample(texts=['¬øC√≥mo se relaciona la ciencia ciudadana con la ciencia abierta?', 'La ciencia ciudadana es una faceta de la ciencia abierta que permite la participaci√≥n del p√∫blico en general en proyectos de investigaci√≥n cient√≠fica.'], label=1.0),
    InputExample(texts=['¬øQu√© es la evaluaci√≥n abierta?', 'La evaluaci√≥n abierta se refiere a la revisi√≥n por pares y la evaluaci√≥n de la investigaci√≥n que es transparente y participativa.'], label=1.0),
    InputExample(texts=['Repositorios de acceso abierto', 'Los repositorios son plataformas donde los investigadores pueden depositar sus publicaciones y datos para que est√©n disponibles en acceso abierto.'], label=1.0),
    InputExample(texts=['Licencias Creative Commons', 'Las licencias Creative Commons son un conjunto de herramientas legales que permiten a los creadores compartir su trabajo de forma abierta, definiendo c√≥mo otros pueden usar, distribuir y modificar sus obras.'], label=1.0),
    InputExample(texts=['Plan de Gesti√≥n de Datos (PGD)', 'Un PGD es un documento formal que describe c√≥mo se gestionar√°n los datos de investigaci√≥n durante y despu√©s de un proyecto, incluyendo su recopilaci√≥n, organizaci√≥n, almacenamiento, acceso y preservaci√≥n.'], label=1.0),
    InputExample(texts=['Identificadores persistentes (PIDs)', 'Los PIDs son identificadores √∫nicos y permanentes utilizados para referenciar objetos digitales (como datasets, publicaciones o investigadores) de manera consistente a lo largo del tiempo, facilitando su descubrimiento y citaci√≥n.'], label=1.0),
    InputExample(texts=['Software de c√≥digo abierto en investigaci√≥n', 'El software de c√≥digo abierto es fundamental para la reproducibilidad y transparencia en la investigaci√≥n, permitiendo a otros usuarios acceder, modificar y ejecutar el c√≥digo utilizado en un estudio.'], label=1.0),
    InputExample(texts=['M√©tricas alternativas (Altmetrics)', 'Las altmetrics son m√©tricas complementarias a las citas tradicionales que miden el impacto de la investigaci√≥n en l√≠nea, como menciones en redes sociales, blogs, noticias o repositorios.'], label=1.0),
    InputExample(texts=['¬øCu√°l es la diferencia entre acceso abierto y datos abiertos?', 'El acceso abierto se centra principalmente en que las publicaciones de investigaci√≥n est√©n libremente disponibles, mientras que los datos abiertos se refieren a hacer que los datos subyacentes a la investigaci√≥n sean accesibles y reutilizables.'], label=0.9),
    InputExample(texts=['Repositorio de datos vs Repositorio de publicaciones', 'Un repositorio de datos almacena datasets de investigaci√≥n, mientras que un repositorio de publicaciones almacena art√≠culos, preprints u otros textos acad√©micos.'], label=0.9),
    InputExample(texts=['Peer review tradicional vs evaluaci√≥n abierta', 'La revisi√≥n por pares tradicional suele ser ciega o doblemente ciega, mientras que la evaluaci√≥n abierta busca mayor transparencia en el proceso, a veces revelando la identidad de los revisores y los comentarios.'], label=0.9),
    InputExample(texts=['Licencia CC BY vs CC BY-SA', 'Ambas son licencias Creative Commons que requieren atribuci√≥n, pero CC BY-SA (ShareAlike) adem√°s exige que las obras derivadas se distribuyan bajo la misma licencia.'], label=0.9),
    InputExample(texts=['¬øQu√© significa reproducible?', 'La reproducibilidad en ciencia implica que un investigador, usando los mismos datos y c√≥digo que el investigador original, puede obtener los mismos resultados.'], label=1.0),
    InputExample(texts=['Plataforma de ciencia ciudadana', 'Ejemplos de plataformas incluyen Zooniverse, donde los voluntarios ayudan a analizar grandes conjuntos de datos.'], label=0.8),
    InputExample(texts=['Identificador ORCID', 'ORCID es un identificador persistente para investigadores, ayudando a distinguirlos y vincular su producci√≥n cient√≠fica.'], label=0.8),
    InputExample(texts=['Preprint', 'Un preprint es una versi√≥n de un art√≠culo de investigaci√≥n que se publica antes de ser revisado por pares en una revista formal.'], label=1.0),
    InputExample(texts=['Ciencia abierta en Am√©rica Latina', 'La ciencia abierta en Am√©rica Latina enfrenta desaf√≠os y oportunidades √∫nicas relacionadas con la infraestructura, las pol√≠ticas y la cultura acad√©mica regional.'], label=1.0),
    InputExample(texts=['¬øC√≥mo define la UNESCO la ciencia abierta?', 'La ciencia abierta se define como un constructo inclusivo que combina diversos movimientos y pr√°cticas con el fin de que los conocimientos cient√≠ficos multiling√ºes est√©n abiertamente disponibles y sean accesibles para todos, as√≠ como reutilizables por todos, se incrementen las colaboraciones cient√≠ficas y el intercambio de informaci√≥n en beneficio de la ciencia y la sociedad, y se abran los procesos de creaci√≥n, evaluaci√≥n y comunicaci√≥n de los conocimientos cient√≠ficos a los agentes sociales m√°s all√° de la comunidad cient√≠fica tradicional.'], label=1.0),
    InputExample(texts=['¬øCu√°les son los pilares fundamentales de la ciencia abierta seg√∫n la UNESCO?', 'La ciencia abierta se basa en los siguientes pilares clave: conocimiento cient√≠fico abierto, infraestructuras de la ciencia abierta, comunicaci√≥n cient√≠fica, participaci√≥n abierta de los agentes sociales y di√°logo abierto con otros sistemas de conocimiento.'], label=1.0)
]

# Crear conjunto de evaluaci√≥n (usando una porci√≥n de los datos de ejemplo) Utilizo 70/30
# En un escenario real, deber√≠as tener un conjunto de evaluaci√≥n separado
eval_examples = examples[-6:]  # √öltimos 6 ejemplos para evaluaci√≥n, es decir utilizo casi el 30% del conjunto de ejemplos
train_examples = examples[:-6]  # Resto para entrenamiento, es decir, el 70 %.

def evaluate_model_metrics(model, eval_examples, threshold=0.8):
    """
    Eval√∫a el modelo calculando m√©tricas de clasificaci√≥n basadas en similitud coseno
    """
    print("\n" + "="*60)
    print("EVALUACI√ìN DE M√âTRICAS DEL MODELO")
    print("="*60)

    true_labels = []
    predicted_labels = []
    similarities = []

    for example in eval_examples:
        # Obtener embeddings de ambos textos
        embedding1 = model.encode([example.texts[0]])
        embedding2 = model.encode([example.texts[1]])

        # Calcular similitud coseno
        similarity = cosine_similarity(embedding1, embedding2)[0][0]
        similarities.append(similarity)

        # Convertir a etiqueta binaria usando threshold
        predicted_label = 1 if similarity >= threshold else 0
        true_label = 1 if example.label >= threshold else 0

        predicted_labels.append(predicted_label)
        true_labels.append(true_label)

        print(f"Texto 1: {example.texts[0][:50]}...")
        print(f"Texto 2: {example.texts[1][:50]}...")
        print(f"Similitud real: {example.label:.3f} | Similitud predicha: {similarity:.3f}")
        print(f"Etiqueta real: {true_label} | Etiqueta predicha: {predicted_label}")
        print("-" * 40)

    # Calcular m√©tricas
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision, recall, f1, support = precision_recall_fscore_support(
        true_labels, predicted_labels, average='binary', zero_division=0
    )

    # Mostrar m√©tricas principales
    print(f"\nüìä M√âTRICAS DE RENDIMIENTO:")
    print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
    print(f"Recall:    {recall:.4f} ({recall*100:.2f}%)")
    print(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)")

    # Estad√≠sticas de similitud
    similarities = np.array(similarities)
    print(f"\nüìà ESTAD√çSTICAS DE SIMILITUD:")
    print(f"Similitud promedio:    {np.mean(similarities):.4f}")
    print(f"Desviaci√≥n est√°ndar:   {np.std(similarities):.4f}")
    print(f"Similitud m√≠nima:      {np.min(similarities):.4f}")
    print(f"Similitud m√°xima:      {np.max(similarities):.4f}")

    # Reporte de clasificaci√≥n detallado
    print(f"\nüìã REPORTE DE CLASIFICACI√ìN DETALLADO:")
    print(classification_report(true_labels, predicted_labels,
                              target_names=['Baja similitud', 'Alta similitud'],
                              zero_division=0))

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'similarities': similarities,
        'true_labels': true_labels,
        'predicted_labels': predicted_labels
    }

def compare_models_performance(original_model, fine_tuned_model, eval_examples):
    """
    Compara el rendimiento entre el modelo original y el fine-tuned
    """
    print("\n" + "="*60)
    print("COMPARACI√ìN DE MODELOS")
    print("="*60)

    print("Evaluando modelo ORIGINAL...")
    original_metrics = evaluate_model_metrics(original_model, eval_examples)

    print("\nEvaluando modelo FINE-TUNED...")
    finetuned_metrics = evaluate_model_metrics(fine_tuned_model, eval_examples)

    # Crear tabla comparativa
    comparison_data = {
        'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Similitud Promedio'],
        'Modelo Original': [
            f"{original_metrics['accuracy']:.4f}",
            f"{original_metrics['precision']:.4f}",
            f"{original_metrics['recall']:.4f}",
            f"{original_metrics['f1_score']:.4f}",
            f"{np.mean(original_metrics['similarities']):.4f}"
        ],
        'Modelo Fine-tuned': [
            f"{finetuned_metrics['accuracy']:.4f}",
            f"{finetuned_metrics['precision']:.4f}",
            f"{finetuned_metrics['recall']:.4f}",
            f"{finetuned_metrics['f1_score']:.4f}",
            f"{np.mean(finetuned_metrics['similarities']):.4f}"
        ]
    }

    df_comparison = pd.DataFrame(comparison_data)
    print("\nüìä TABLA COMPARATIVA:")
    print(df_comparison.to_string(index=False))

    # Calcular mejoras
    accuracy_improvement = finetuned_metrics['accuracy'] - original_metrics['accuracy']
    f1_improvement = finetuned_metrics['f1_score'] - original_metrics['f1_score']

    print(f"\nüöÄ MEJORAS OBTENIDAS:")
    print(f"Mejora en Accuracy: {accuracy_improvement:+.4f} ({accuracy_improvement*100:+.2f}%)")
    print(f"Mejora en F1-Score: {f1_improvement:+.4f} ({f1_improvement*100:+.2f}%)")

# Cargar el modelo original para comparaci√≥n
print("Cargando modelo original...")
original_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

# Evaluar modelo original ANTES del fine-tuning
print("Evaluando modelo ANTES del fine-tuning...")
original_metrics = evaluate_model_metrics(original_model, eval_examples)

# Load the model para fine-tuning
modelFT = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

# Create a DataLoader
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# Define the loss function
train_loss = losses.CosineSimilarityLoss(model=modelFT)

# Define number of epochs
num_epochs = 2

# Crear evaluador para seguimiento durante el entrenamiento
evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(
    eval_examples, name='eval'
)

# Fine-tuning con evaluaci√≥n
print(f"\nüîÑ Iniciando fine-tuning del modelo de embeddings por {num_epochs} √©poca(s)...")
print("="*60)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=num_epochs,
    warmup_steps=100,
    evaluator=evaluator,
    evaluation_steps=50,  # Evaluar cada 50 pasos
    output_path="./fine_tuned_model"
)

print("\n‚úÖ Fine-tuning completado!")

# Evaluar modelo DESPU√âS del fine-tuning
print("\nEvaluando modelo DESPU√âS del fine-tuning...")
finetuned_metrics = evaluate_model_metrics(modelFT, eval_examples)

# Comparar rendimiento
compare_models_performance(original_model, modelFT, eval_examples)

# Guardar el modelo
modelFT.save("fine_tuned_model")
print(f"\nüíæ Modelo guardado en: ./fine_tuned_model")

# Funci√≥n adicional para evaluar en nuevos ejemplos
def test_model_on_new_examples(model, new_examples):
    """
    Funci√≥n para probar el modelo en nuevos ejemplos
    """
    print("\n" + "="*60)
    print("PRUEBA CON NUEVOS EJEMPLOS")
    print("="*60)

    for i, (text1, text2) in enumerate(new_examples, 1):
        embedding1 = modelFT.encode([text1])
        embedding2 = modelFT.encode([text2])
        similarity = cosine_similarity(embedding1, embedding2)[0][0]

        print(f"Ejemplo {i}:")
        print(f"Texto 1: {text1}")
        print(f"Texto 2: {text2}")
        print(f"Similitud: {similarity:.4f}")
        print("-" * 40)

# Ejemplo de uso con nuevos textos
nuevos_ejemplos = [
    ("¬øQu√© son los datos abiertos?", "Los datos abiertos son datos que pueden ser utilizados, reutilizados y redistribuidos libremente por cualquier persona."),
    ("Pol√≠tica de ciencia abierta", "Las pol√≠ticas institucionales promueven pr√°cticas de acceso abierto en las universidades."),
    ("Gatos y perros", "Los datos FAIR son importantes para la investigaci√≥n reproducible.")
]

print(f"\nProbando modelo fine-tuned con nuevos ejemplos...")
test_model_on_new_examples(modelFT, nuevos_ejemplos)

"""# **A. EN ETAPA DE DESARROLLO: Base de Conocimiento Estructurada**
### Nota: Ver la notebook base_conocimiento_en_desarrollo dentro del mismo repositorio.

## A.1 Taxonom√≠a de Pilares de Ciencia Abierta

Para la definici√≥n de la taxonom√ça utilizada en el entrenamiento se revisaron los siguientes documentos, recomendaciones de CA de la UNESCO (2021) y  Taxonomia da Ci√™ncia Aberta: revisada e ampliada, adem√°s del material del su curso de OpenScience 101 desarrollado por por Nasa. En estos se identificaron distintas taxonom√≠as como Pontika et al. (2015), Baumgartner (2019), Silveira et al. (2021), y una √∫ltima ampliada. Finalmente, se decidi√≥ establecer una clasificaci√≥n basada en las recomendaciones de la Unesco para la ciencia abierta y en los m√≥dulos definidos en el curso de OpenScience 101. La misma consiste en: acceso_abierto.datos abiertos, codigo_abierto, evaluacion_abierta y ciencia_ciudadana.

Fuente: Art√≠culo: SciELO Brasil - Taxonomia da Ci√™ncia Aberta: revisada e ampliada Taxonomia da Ci√™ncia Aberta: revisada e ampliada https://share.google/C0r8ikdeLFwAGjR2u.
"""

PILARES_CA = {
    "acceso_abierto": {
        "subcategorias": ["publicaciones", "repositorios", "preprints"],
        "entidades": ["revistas", "editoriales", "licencias"]
    },
    "datos_abiertos": {
        "subcategorias": ["FAIR", "repositorios", "metadatos"],
        "entidades": ["formatos", "identificadores", "planes_gestion"]
    },
    "codigo_abierto": {
        "subcategorias": ["software", "notebooks", "reproducibilidad"],
        "entidades": ["licencias", "plataformas", "versionado", "lenguaje"]
    },
    "evaluacion_abierta": {
        "subcategorias": ["peer_review", "metricas", "transparencia"],
        "entidades": ["altmetrics", "sistemas_evaluacion"]
    },
    "ciencia_ciudadana": {
        "subcategorias": ["participacion", "metodologias", "plataformas"],
        "entidades": ["proyectos", "herramientas", "comunidades"]
    }
}

"""# A.2 Estructura de Pares Pregunta-Respuesta"""

class QAPair:
    def __init__(self):
        self.pregunta = ""
        self.respuesta = ""
        self.pilar = ""
        self.nivel_complejidad = ""  # b√°sico, intermedio, avanzado
        self.audiencia = ""  # maestr√≠a, doctorado, investigador
        self.contexto_regional = ""  # Am√©rica Latina espec√≠fico
        self.fuentes = []
        self.entidades_relacionadas = []

"""## 4.2 Integraci√≥n y Evaluaci√≥n de Modelos LLM"""