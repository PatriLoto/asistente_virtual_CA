{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNxpUzZ/ChmggEyNaSa2iTG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install PyPDF2 pdfplumber nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrWrTXWpOWrm","executionInfo":{"status":"ok","timestamp":1753793286892,"user_tz":180,"elapsed":15292,"user":{"displayName":"Patricia Loto","userId":"02565787132075250303"}},"outputId":"41093b3c-e76a-4828-d518-b71cba278ea6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting pdfplumber\n","  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Collecting pdfminer.six==20250506 (from pdfplumber)\n","  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n","Collecting pypdfium2>=4.18.0 (from pdfplumber)\n","  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n","Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdfium2, PyPDF2, pdfminer.six, pdfplumber\n","Successfully installed PyPDF2-3.0.1 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"]}]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3WW0KOmKpsi","executionInfo":{"status":"ok","timestamp":1753808436910,"user_tz":180,"elapsed":60,"user":{"displayName":"Patricia Loto","userId":"02565787132075250303"}},"outputId":"b426ad92-b026-4836-e0f2-10a73704b888"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import os\n","import re\n","import PyPDF2\n","import pdfplumber\n","from pathlib import Path\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","\n","# Descargar recursos de NLTK si no están disponibles\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","    nltk.data.find('tokenizers/pupunkt_tabnkt')\n","except LookupError:\n","    nltk.download('punkt')\n","    nltk.download('punkt_tab')\n","\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')\n","\n","class PDFProcessor:\n","    def __init__(self, input_folder, output_folder):\n","        \"\"\"\n","        Inicializa el procesador de PDFs\n","\n","        Args:\n","            input_folder: Carpeta con los PDFs a procesar\n","            output_folder: Carpeta donde se guardarán los archivos procesados\n","        \"\"\"\n","        self.input_folder = Path(input_folder)\n","        self.output_folder = Path(output_folder)\n","        self.output_folder.mkdir(exist_ok=True)\n","\n","        # Configurar stopwords en español\n","        self.stop_words = set(stopwords.words('spanish'))\n","\n","    def extract_text_from_pdf(self, pdf_path):\n","        \"\"\"\n","        Extrae texto de un archivo PDF usando pdfplumber como método principal\n","        y PyPDF2 como respaldo\n","        \"\"\"\n","        text = \"\"\n","\n","        try:\n","            # Intentar con pdfplumber primero (mejor para PDFs complejos)\n","            with pdfplumber.open(pdf_path) as pdf:\n","                for page in pdf.pages:\n","                    page_text = page.extract_text()\n","                    if page_text:\n","                        text += page_text + \"\\n\"\n","        except:\n","            # Si falla, intentar con PyPDF2\n","            try:\n","                with open(pdf_path, 'rb') as file:\n","                    pdf_reader = PyPDF2.PdfReader(file)\n","                    for page_num in range(len(pdf_reader.pages)):\n","                        page = pdf_reader.pages[page_num]\n","                        text += page.extract_text() + \"\\n\"\n","            except Exception as e:\n","                print(f\"Error al leer {pdf_path}: {str(e)}\")\n","                return \"\"\n","\n","        return text\n","\n","    def clean_text(self, text):\n","        \"\"\"\n","        Limpia el texto según los criterios especificados\n","        \"\"\"\n","        # Convertir a minúsculas\n","        text = text.lower()\n","\n","        # Eliminar números de página (patrones comunes)\n","        # Elimina números solos en una línea o números con formato \"Página X\"\n","        text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n","        text = re.sub(r'página\\s*\\d+', '', text, flags=re.IGNORECASE)\n","        text = re.sub(r'pp' '\\s*\\d+', '', text, flags=re.IGNORECASE)\n","        text = re.sub(r'p\\.\\s*\\d+', '', text, flags=re.IGNORECASE)\n","        text = re.sub(r'pág\\.\\s*\\d+', '', text, flags=re.IGNORECASE)\n","        text = re.sub(r'^-\\s*\\d+\\s*-\\s*$', '', text, flags=re.MULTILINE)\n","\n","        # También manejar casos donde hay espacios antes del salto de línea\n","        # Como \"nac- ieron\" en la misma línea\n","        text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n","\n","        # Eliminar patrones tipo peerj.preprints.27580v1 o similares\n","        # Este patrón captura: palabra.palabra.número+letra+número\n","        text = re.sub(r'\\b\\w+\\.\\w+\\.\\d+[a-zA-Z]+\\d*\\b', ' ', text)\n","\n","        # Eliminar patrones de identificadores de documentos científicos\n","        # Como: arxiv.1234.5678, doi.10.1234/5678, pmid.12345678\n","        text = re.sub(r'\\b\\w+\\.\\d+\\.\\d+\\b', ' ', text)\n","        text = re.sub(r'\\bdoi\\.\\borg\\.\\S+', ' ', text)\n","        text = re.sub(r'\\bpmid\\.\\d+', ' ', text)\n","        text = re.sub(r'\\barxiv\\.\\d+\\.\\d+', ' ', text)\n","\n","\n","        # Eliminar números con comas como 45, 56, 78\n","        # Este patrón elimina secuencias de números separados por comas\n","        text = re.sub(r'\\b\\d+(?:\\s*,\\s*\\d+)+\\b', '', text)\n","\n","        # Eliminar números solos (incluyendo decimales)\n","        # Esto elimina números aislados como 123, 45.67, etc.\n","        text = re.sub(r'\\b\\d+\\.?\\d*\\b', '', text)\n","\n","        # Eliminar referencias tipo [1], [2,3], [45-47], etc.\n","        text = re.sub(r'\\[\\d+(?:[-,]\\d+)*\\]', '', text)\n","\n","        # Eliminar años solos (4 dígitos)\n","        text = re.sub(r'\\b\\d{4}\\b', '', text)\n","\n","        # casos especiales de limpieza del texto de UNESCO\n","        #Eliminar números romanos en minúscula\n","        #roman_numerals_pattern = r'\\b(?:i[vx]|v?i{0,3}|x)\\b'\n","        text = re.sub(r'\\b(?:i[vx]|v?i{0,3}|x)\\b', ' ', text)\n","        # elimina numeracion del tipo 45ª\n","        text = re.sub(r'\\b\\d+ª\\b', ' ', text)\n","        #elimina url que solo terminan con '.com'\n","        text = re.sub(r'\\b(?!https:\\/\\/|www\\.)[a-zA-Z0-9.-]+\\.com\\b', ' ', text)\n","\n","\n","        # casos especiales de limpieza del texto de Conocimiento abierto\n","        # elimina letras solas\n","        text = re.sub(r'\\b[a-z]\\b', ' ', text)\n","        # elimina el siguiente patrón: ccoolleecccciióónn ggrruuppooss ddee ttrraabbaajjoo ccoonnoocciimmiieennttoo aabbiieerrttoo eenn aamméérriiccaa llaattiinnaa\n","        text = re.sub(r'([a-záéíóúñ])\\1+',r'\\1', text) # ver si conviene o no, tiene palabras en ingles como access(queda aces) o common(queda comon)\n","        # filtra las palabras de 2 digitos: Ej:fs, ar, eb\n","        text = re.sub(r'\\b[a-z]{2}\\b', ' ', text)\n","        # filtra las palabras de 3 digitos sin vocales: Ej:pcb\n","        text = re.sub(r'\\b[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]{3}\\b', ' ', text)\n","        # filtra las asociaciones de este tipo: n2o0s2, a0i\n","        text = re.sub(r'\\b(?=\\w*\\d)(?=\\d*\\w)\\w+\\b', ' ', text)\n","\n","\n","\n","        # Eliminar URLs\n","        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","        text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","        #text = re.sub(r'[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),%]+(?:\\.[a-zA-Z]{2,})+', '', text)\n","\n","        # Remove HTML tags\n","        text = re.sub ( r'<.*?>' ,'', text)\n","\n","        # Eliminar correos electrónicos\n","        text = re.sub(r'\\S+@\\S+', '', text)\n","\n","        # Eliminar caracteres especiales excesivos pero mantener puntuación básica\n","        # Mantener: . , ; : ! ? ¿ ¡ ( ) - \" '\n","        # text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\¿\\¡\\(\\)\\-\\\"\\ª\\'áéíóúñÁÉÍÓÚÑ]', '', text)\n","        # decido eliminar todos los caracteres especiales por eso comento lo anterior\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","\n","        # Eliminar múltiples espacios\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        # Eliminar múltiples saltos de línea\n","        text = re.sub(r'\\n+', '\\n', text)\n","\n","        # Eliminar líneas que solo contienen espacios\n","        lines = text.split('\\n')\n","        lines = [line.strip() for line in lines if line.strip()]\n","        text = '\\n'.join(lines)\n","\n","        return text\n","\n","    # def tokenize_text(self, text, remove_stopwords=True, remove_punctuation=True):\n","    #     \"\"\"\n","    #     Tokeniza el texto y opcionalmente elimina stopwords y puntuación\n","\n","    #     Args:\n","    #         text: Texto a tokenizar\n","    #         remove_stopwords: Si eliminar palabras vacías\n","    #         remove_punctuation: Si eliminar puntuación\n","    #     \"\"\"\n","    #     # Tokenizar\n","    #     tokens = word_tokenize(text, language='spanish')\n","\n","    #     # Filtrar tokens\n","    #     filtered_tokens = []\n","    #     for token in tokens:\n","    #         # Eliminar puntuación si se especifica\n","    #         if remove_punctuation and token in string.punctuation:\n","    #             continue\n","\n","    #         # Eliminar stopwords si se especifica\n","    #         if remove_stopwords and token.lower() in self.stop_words:\n","    #             continue\n","\n","    #         # Eliminar tokens de un solo carácter (excepto 'a' y 'o' que pueden ser significativos)\n","    #         if len(token) == 1 and token.lower() not in ['a', 'o']:\n","    #             continue\n","\n","    #         # Eliminar tokens que son solo números\n","    #         if token.isdigit():\n","    #             continue\n","\n","    #         filtered_tokens.append(token)\n","\n","    #     return filtered_tokens\n","\n","\n","    def tokenize_text(self, text, remove_punctuation=True):\n","      \"\"\"\n","      Tokeniza el texto y siempre elimina stopwords del español y números/dígitos solos.\n","      Opcionalmente, elimina la puntuación.\n","\n","      Args:\n","          text (str): Texto a tokenizar.\n","          remove_punctuation (bool): Si eliminar puntuación (True por defecto).\n","\n","      Returns:\n","          list: Lista de tokens limpios.\n","      \"\"\"\n","\n","      # Tokenizar el texto\n","      tokens = word_tokenize(text, language='spanish')\n","\n","      ## Convertir a minúsculas para una comparación consistente\n","      # ya lo hago en la sección de limpieza\n","      #tokens = [token.lower() for token in tokens]\n","\n","      filtered_tokens = []\n","      for token in tokens:\n","          # Eliminar stopwords de español (siempre)\n","          if token in self.stop_words:\n","              continue\n","\n","          # Eliminar números/dígitos solos (siempre)\n","          if token.isdigit():\n","              continue\n","\n","          # Eliminar tokens de un solo carácter (excepto 'a' y 'o' que pueden ser significativos)\n","          # Ya que eliminamos puntuación antes, esto se enfoca más en letras sueltas\n","          if len(token) == 1 and token not in ['a', 'o']:\n","              continue\n","\n","          # Asegurarse de no añadir cadenas vacías resultantes de la limpieza previa\n","          if token:\n","              filtered_tokens.append(token)\n","\n","      return filtered_tokens\n","\n","\n","\n","\n","\n","    def save_processed_text(self, text, tokens, filename):\n","        \"\"\"\n","        Guarda el texto procesado y los tokens en archivos separados\n","        \"\"\"\n","        # Crear subcarpetas\n","        text_folder = self.output_folder / 'cleaned_text'\n","        tokens_folder = self.output_folder / 'tokens'\n","        text_folder.mkdir(exist_ok=True)\n","        tokens_folder.mkdir(exist_ok=True)\n","\n","        # Guardar texto limpio\n","        text_path = text_folder / f\"{filename}_cleaned.txt\"\n","        with open(text_path, 'w', encoding='utf-8') as f:\n","            f.write(text)\n","\n","        # Guardar tokens\n","        tokens_path = tokens_folder / f\"{filename}_tokens.txt\"\n","        with open(tokens_path, 'w', encoding='utf-8') as f:\n","            f.write(' '.join(tokens))\n","\n","        # Guardar tokens uno por línea (útil para análisis)\n","        tokens_lines_path = tokens_folder / f\"{filename}_tokens_lines.txt\"\n","        with open(tokens_lines_path, 'w', encoding='utf-8') as f:\n","            for token in tokens:\n","                f.write(token + '\\n')\n","\n","        return text_path, tokens_path\n","\n","    def process_single_pdf(self, pdf_path):\n","        \"\"\"\n","        Procesa un único archivo PDF\n","        \"\"\"\n","        print(f\"\\nProcesando: {pdf_path.name}\")\n","\n","        # Extraer texto\n","        print(\"  - Extrayendo texto...\")\n","        raw_text = self.extract_text_from_pdf(pdf_path)\n","\n","        if not raw_text:\n","            print(f\"  - ERROR: No se pudo extraer texto de {pdf_path.name}\")\n","            return None\n","\n","        # Limpiar texto\n","        print(\"  - Limpiando texto...\")\n","        cleaned_text = self.clean_text(raw_text)\n","\n","        # Tokenizar\n","        print(\"  - Tokenizando...\")\n","        tokens = self.tokenize_text(cleaned_text, remove_punctuation=True)\n","        #tokens = self.tokenize_text(cleaned_text, remove_stopwords=True, remove_punctuation=True)\n","\n","        # Guardar resultados\n","        print(\"  - Guardando resultados...\")\n","        filename = pdf_path.stem  # Nombre sin extensión\n","        text_path, tokens_path = self.save_processed_text(cleaned_text, tokens, filename)\n","\n","        # Mostrar estadísticas\n","        print(f\"  - Estadísticas:\")\n","        print(f\"    * Caracteres en texto original: {len(raw_text)}\")\n","        print(f\"    * Caracteres en texto limpio: {len(cleaned_text)}\")\n","        print(f\"    * Número de tokens: {len(tokens)}\")\n","        print(f\"    * Tokens únicos: {len(set(tokens))}\")\n","\n","        return {\n","            'filename': filename,\n","            'original_chars': len(raw_text),\n","            'cleaned_chars': len(cleaned_text),\n","            'total_tokens': len(tokens),\n","            'unique_tokens': len(set(tokens)),\n","            'text_path': text_path,\n","            'tokens_path': tokens_path\n","        }\n","\n","    def process_all_pdfs(self):\n","        \"\"\"\n","        Procesa todos los PDFs en la carpeta de entrada\n","        \"\"\"\n","        # Buscar todos los archivos PDF\n","        pdf_files = list(self.input_folder.glob('*.pdf'))\n","\n","        if not pdf_files:\n","            print(f\"No se encontraron archivos PDF en {self.input_folder}\")\n","            return []\n","\n","        print(f\"Se encontraron {len(pdf_files)} archivos PDF para procesar\")\n","\n","        results = []\n","        for pdf_path in pdf_files:\n","            result = self.process_single_pdf(pdf_path)\n","            if result:\n","                results.append(result)\n","\n","        # Generar resumen\n","        self.generate_summary(results)\n","\n","        return results\n","\n","    def generate_summary(self, results):\n","        \"\"\"\n","        Genera un resumen del procesamiento\n","        \"\"\"\n","        if not results:\n","            return\n","\n","        summary_path = self.output_folder / 'processing_summary.txt'\n","\n","        with open(summary_path, 'w', encoding='utf-8') as f:\n","            f.write(\"RESUMEN DE PROCESAMIENTO DE PDFs\\n\")\n","            f.write(\"=\" * 50 + \"\\n\\n\")\n","            f.write(f\"Total de archivos procesados: {len(results)}\\n\\n\")\n","\n","            total_original = sum(r['original_chars'] for r in results)\n","            total_cleaned = sum(r['cleaned_chars'] for r in results)\n","            total_tokens = sum(r['total_tokens'] for r in results)\n","\n","            f.write(f\"Estadísticas globales:\\n\")\n","            f.write(f\"  - Caracteres totales (original): {total_original:,}\\n\")\n","            f.write(f\"  - Caracteres totales (limpio): {total_cleaned:,}\\n\")\n","            f.write(f\"  - Reducción de caracteres: {(1 - total_cleaned/total_original)*100:.1f}%\\n\")\n","            f.write(f\"  - Tokens totales: {total_tokens:,}\\n\\n\")\n","\n","            f.write(\"Detalle por archivo:\\n\")\n","            f.write(\"-\" * 50 + \"\\n\")\n","\n","            for r in results:\n","                f.write(f\"\\n{r['filename']}.pdf\\n\")\n","                f.write(f\"  - Caracteres originales: {r['original_chars']:,}\\n\")\n","                f.write(f\"  - Caracteres limpios: {r['cleaned_chars']:,}\\n\")\n","                f.write(f\"  - Tokens totales: {r['total_tokens']:,}\\n\")\n","                f.write(f\"  - Tokens únicos: {r['unique_tokens']:,}\\n\")\n","\n","        print(f\"\\nResumen guardado en: {summary_path}\")\n","\n","def calculate_word_frequencies(tokens_file, top_n=50):\n","    \"\"\"\n","    Calcula las frecuencias de palabras desde un archivo de tokens\n","    \"\"\"\n","    from collections import Counter\n","\n","    with open(tokens_file, 'r', encoding='utf-8') as f:\n","        tokens = f.read().split()\n","\n","    # Calcular frecuencias\n","    word_freq = Counter(tokens)\n","\n","    # Obtener las palabras más comunes\n","    most_common = word_freq.most_n(top_n)\n","\n","    return word_freq, most_common"]},{"cell_type":"code","source":["#import nltk\n","#nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5lCw0471Ovww","executionInfo":{"status":"ok","timestamp":1753802147760,"user_tz":180,"elapsed":120,"user":{"displayName":"Patricia Loto","userId":"02565787132075250303"}},"outputId":"d2b5561b-76d2-4ebe-9e72-2010f1142c55"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Ejemplo de uso\n","if __name__ == \"__main__\":\n","    # Configurar rutas\n","    input_folder = \"pdfs\"  # Carpeta con los PDFs\n","    output_folder = \"processed_pdfs\"  # Carpeta de salida\n","\n","    # Crear procesador\n","    processor = PDFProcessor(input_folder, output_folder)\n","\n","    # Procesar todos los PDFs\n","    results = processor.process_all_pdfs()\n","\n","    # Ejemplo: Calcular frecuencias de palabras del primer archivo procesado\n","    if results:\n","        first_result = results[0]\n","        tokens_file = first_result['tokens_path']\n","\n","        print(f\"\\n\\nFrecuencias de palabras para {first_result['filename']}:\")\n","        print(\"-\" * 50)\n","\n","        word_freq, most_common = calculate_word_frequencies(tokens_file, top_n=20)\n","\n","        for word, count in most_common:\n","            print(f\"{word}: {count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":852},"id":"PRNKSHmnMjNR","executionInfo":{"status":"error","timestamp":1753808487747,"user_tz":180,"elapsed":36512,"user":{"displayName":"Patricia Loto","userId":"02565787132075250303"}},"outputId":"03f08942-ec12-435d-bbea-15827d791331"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Se encontraron 2 archivos PDF para procesar\n","\n","Procesando: Recomendación de la UNESCO sobre la Ciencia Abierta.pdf\n","  - Extrayendo texto...\n","  - Limpiando texto...\n","  - Tokenizando...\n","  - Guardando resultados...\n","  - Estadísticas:\n","    * Caracteres en texto original: 77715\n","    * Caracteres en texto limpio: 65808\n","    * Número de tokens: 5981\n","    * Tokens únicos: 1724\n","\n","Procesando: Conocimiento_abierto_en_america_latina_trayectorias_y_desafios.pdf\n","  - Extrayendo texto...\n","  - Limpiando texto...\n","  - Tokenizando...\n","  - Guardando resultados...\n","  - Estadísticas:\n","    * Caracteres en texto original: 622410\n","    * Caracteres en texto limpio: 499897\n","    * Número de tokens: 49546\n","    * Tokens únicos: 9299\n","\n","Resumen guardado en: processed_pdfs/processing_summary.txt\n","\n","\n","Frecuencias de palabras para Recomendación de la UNESCO sobre la Ciencia Abierta:\n","--------------------------------------------------\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'Counter' object has no attribute 'most_n'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-23-570353209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_common\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_word_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_common\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-22-503882862.py\u001b[0m in \u001b[0;36mcalculate_word_frequencies\u001b[0;34m(tokens_file, top_n)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;31m# Obtener las palabras más comunes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0mmost_common\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Counter' object has no attribute 'most_n'"]}]}]}